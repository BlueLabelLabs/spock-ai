{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["spock_dataset  = load_dataset(\"csv\", data_files=\"/home/bobby/projects/bll-ai-toolbox-model-1/datasets/_scripts_TOS_cleaned.csv\")\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['title', 'original_airdate', 'production_number', 'dialogue'],\n","        num_rows: 3476\n","    })\n","    validation: Dataset({\n","        features: ['title', 'original_airdate', 'production_number', 'dialogue'],\n","        num_rows: 869\n","    })\n","})\n"]}],"source":["\n","#drop all the columns except dialogue\n","spock_dataset = spock_dataset.map(lambda x: {\"dialogue\": x[\"dialogue\"]})\n","#split the dataset into a training and validation set\n","train_dataset = spock_dataset[\"train\"]\n","dataset_split = train_dataset.train_test_split(test_size=0.2,seed=22)\n","#Rename the default 'test' split to 'validation'\n","dataset_split['validation'] = dataset_split.pop('test')\n","print(dataset_split)\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#subset_size=100\n","#the following is temporary code to help debug training issues by choosing a small subset to work with\n","#train_subset = train_dataset.select(range(subset_size))\n","#validation_subset = dataset_split['validation'].select(range(subset_size))\n","\n","#Create a new DatasetDict with the subsets\n","#dataset_split = DatasetDict({'train': train_subset, 'validation': validation_subset})\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["  ## Parameters"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["pretrained_model = \"distilbert/distilgpt2\"\n","finetuned_modelname = \"distilgpt2-spock\"\n","huggingface_username = \"omgbobbyg\"\n","huggingface_reponame = f\"{huggingface_username}/{finetuned_modelname}\"  \n"]},{"cell_type":"markdown","metadata":{},"source":["  ## Tokenization"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Maximum context size: 1024\n"]}],"source":["from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, AutoModelForCausalLM\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n","# Get the maximum context size\n","max_length = model.config.max_position_embeddings\n","print(f\"Maximum context size: {max_length}\")\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from transformers import GPT2Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n","\n","# if tokenizer.pad_token is None:\n","#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","# Define a function to tokenize the text data\n","#def tokenize_function(examples):\n","#    return tokenizer(examples[\"dialogue\"], max_length=max_length, truncation=True, padding=True)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"dialogue\"],max_length=max_length)\n","\n","\n","\n","\n","# Apply the tokenization function to the entire dataset\n","tokenized_datasets = dataset_split.map(\n","    tokenize_function,\n","    batched=True,\n","    batch_size=10,\n","    remove_columns=dataset_split[\"train\"].column_names\n",")\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#Verify that none of our tokenized inputs are greater the maximum context size\n","# Example: Checking the length of the first few tokenized inputs\n","# for i, input_ids in enumerate(tokenized_datasets['train'][range(5)]):\n","#     print(f\"Length of input {i}: {len(input_ids)}\")\n","#     assert len(input_ids) <= max_length, f\"Input {i} exceeds the maximum context size.\"\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["input_ids shape: torch.Size([1, 8])\n","attention_mask shape: torch.Size([1, 8])\n","labels shape: torch.Size([1, 8])\n"]}],"source":["#We need to create data collator to manage the batches, we can use DataCollatorForLanguageModeling\n","from transformers import DataCollatorForLanguageModeling\n","tokenizer.pad_token =\"<pad>\" #tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)\n","# Iterate over the generator\n","out = data_collator([tokenized_datasets[\"train\"][i] for i in range(1)])\n","for key in out:\n","    print(f\"{key} shape: {out[key].shape}\")\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10ae4c15733740dfaaa05483fb71e642","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import torch; torch.cuda.is_available()\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import logging    \n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","# Configure the logger if it hasn't been configured before\n","if not logger.handlers:\n","    handler = logging.FileHandler('training.log')\n","    handler.setLevel(logging.INFO)\n","    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","    handler.setFormatter(formatter)\n","    logger.addHandler(handler)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#we define our metrics computation\n","from transformers import EvalPrediction, PreTrainedTokenizer\n","import numpy as np\n","import math\n","\n","def index_of_longest_sequence(labels):\n","    \"\"\"\n","    Finds the index of the longest sequence in the labels array.\n","    \n","    Parameters:\n","    - labels: A 2D numpy.ndarray or a list of lists, where each inner list contains token IDs for one sequence.\n","              Padding tokens are represented by -100.\n","    \n","    Returns:\n","    - An integer representing the index of the longest sequence.\n","    \"\"\"\n","    # Count the number of non-padding tokens (-100) in each sequence\n","    sequence_lengths = np.count_nonzero(labels != -100, axis=1)\n","    \n","    # Find the index of the sequence with the maximum length\n","    longest_sequence_index = np.argmax(sequence_lengths)\n","    \n","    return longest_sequence_index\n","\n","def decode_prediction_row(tokenizer: PreTrainedTokenizer, prediction_logits_row):\n","    \"\"\"\n","    Decodes a single row of prediction logits back to a string.\n","\n","    Parameters:\n","    - tokenizer: An instance of transformers.PreTrainedTokenizer.\n","    - prediction_logits_row: A 2D tensor or array containing logits for a single row of predictions.\n","      The shape is expected to be [sequence_length, vocabulary_size].\n","\n","    Returns:\n","    - A string decoded from the most likely token IDs in prediction_logits_row.\n","    \"\"\"\n","\n","     # Ensure prediction_logits_row is a PyTorch tensor\n","    if isinstance(prediction_logits_row, np.ndarray):\n","        prediction_logits_row = torch.tensor(prediction_logits_row)\n","    \n","    # Convert logits to token IDs by taking the argmax over the vocabulary dimension\n","    token_ids = torch.argmax(prediction_logits_row, dim=-1)\n","    \n","    # Decode the token IDs to a string\n","    decoded_string = tokenizer.decode(token_ids, skip_special_tokens=True)\n","    \n","    return decoded_string\n","\n","def decode_labels_row(tokenizer: PreTrainedTokenizer, labels_row):\n","    \"\"\"\n","    Decodes a single row of labels back to a string, ignoring padding tokens.\n","\n","    Parameters:\n","    - tokenizer: An instance of transformers.PreTrainedTokenizer.\n","    - labels_row: A 1D tensor or array containing token IDs for a single row of labels.\n","\n","    Returns:\n","    - A string decoded from the labels_row, excluding padding tokens.\n","    \"\"\"\n","    # Filter out padding tokens (-100) from the labels row\n","    filtered_tokens = [token for token in labels_row if token != -100]\n","    \n","    # Decode the filtered token IDs to a string\n","    decoded_string = tokenizer.decode(filtered_tokens, skip_special_tokens=True)\n","    \n","    return decoded_string\n","\n","def compute_metrics(eval_pred: EvalPrediction):\n","     # Create or get the logger\n","    logger = logging.getLogger(__name__)\n","    \n","\n","    logits = eval_pred.predictions\n","    labels = eval_pred.label_ids\n","    \n","    index = index_of_longest_sequence(labels)\n","    decoded_prediction = decode_prediction_row(tokenizer, logits[index])\n","    decoded_label = decode_labels_row(tokenizer, labels[index])\n","\n","    shift_logits = logits[..., :-1, :]\n","    shift_labels = labels[..., 1:]\n","    #this code flattens the 3D array to a 2d array by collapsing dimension 1 and 2 together and keeping dimension 3 the same\n","    shift_logits_flat = shift_logits.reshape(-1, shift_logits.shape[-1])\n","    #this equivalent to doing reshape(-1)\n","    shift_labels_flat = shift_labels.flatten()\n","    \n","    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n","    loss = loss_fct(torch.from_numpy(shift_logits_flat), torch.from_numpy(shift_labels_flat))\n","    \n","    num_tokens = np.count_nonzero(shift_labels_flat != -100)\n","    perplexity = math.exp(loss.item() / num_tokens)\n","    logger.info(f\"Perplexity: {perplexity}\")\n","\n","    \n","    # Log the predictions and labels\n","    logger.info(f'Generated Text: {decoded_prediction}')\n","    logger.info(f'Actual Text: {decoded_label}')\n","    logger.info(f\"Perplexity: {perplexity}\")\n","    return {\"perplexity\": perplexity}\n","\n","def compute_metrics_2(eval_pred: EvalPrediction):\n","    logits, labels = eval_pred\n","    # Ensure logits and labels are PyTorch tensors\n","    if not isinstance(logits, torch.Tensor):\n","        logits = torch.tensor(logits)\n","    if not isinstance(labels, torch.Tensor):\n","        labels = torch.tensor(labels)\n","\n","    # Reshape and shift as necessary, foc\n","    # Shift labels to the right to align with model's predictions\n","    # Flatten the logits and labels to calculate loss only for non-ignored indices\n","    #logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])\n","    #labels = labels[..., 1:].flatten()\n","\n","    logits = logits.reshape(-1, logits.shape[-1])\n","    labels = labels.flatten()\n","\n","\n","    # Filter out `-100` used for ignored indices in labels\n","    valid_indices = labels != -100\n","    valid_logits = logits[valid_indices]\n","    valid_labels = labels[valid_indices]\n","\n","    # Calculate Cross Entropy Loss for valid positions\n","    loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n","    loss = loss_fct(valid_logits, valid_labels)\n","\n","    # Calculate Perplexity\n","    perplexity = torch.exp(loss)\n","\n","    return {\"perplexity\": perplexity.item()}\n","\n","    \n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#Now we train the model using the Trainer API\n","from transformers import Trainer, TrainingArguments\n","\n","args = TrainingArguments(\n","    finetuned_modelname,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    fp16=False,\n","    push_to_hub=True,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    hub_model_id=huggingface_reponame\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    args=args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"]\n",")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#lets now run against the base model and log the results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m initial_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(initial_results)\n\u001b[1;32m      4\u001b[0m \u001b[39m#log the results to file\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/trainer.py:3066\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3063\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3065\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3066\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3067\u001b[0m     eval_dataloader,\n\u001b[1;32m   3068\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3069\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3070\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3071\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3072\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3073\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3074\u001b[0m )\n\u001b[1;32m   3076\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3077\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/trainer.py:3245\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3243\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   3244\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3245\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3246\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3248\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:676\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m--> 676\u001b[0m     data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39;49mpin_memory\u001b[39m.\u001b[39;49mpin_memory(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pin_memory_device)\n\u001b[1;32m    677\u001b[0m \u001b[39mreturn\u001b[39;00m data\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:63\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m     62\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)({k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()})  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[39mreturn\u001b[39;00m {k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()}\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:63\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m     62\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)({k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()})  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[39mreturn\u001b[39;00m {k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()}\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:58\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpin_memory\u001b[39m(data, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 58\u001b[0m         \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mpin_memory(device)\n\u001b[1;32m     59\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[1;32m     60\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n","\u001b[0;31mRuntimeError\u001b[0m: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned"]}],"source":["#lets now run against the base model and log the results\n","initial_results = trainer.evaluate()\n","print(initial_results)\n","#log the results to file\n","logger.info(f\"Baseline {pretrained_model} Results: Perplexity: {math.exp(initial_results['eval_loss']):.2f}\")\n","print(f\"Baseline {pretrained_model} Results: Perplexity: {math.exp(initial_results['eval_loss']):.2f}\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","/home/bobby/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n"]},{"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# grab a piece of text from the eval data set to use as a prompt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mIt is your only other choice\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[39m=\u001b[39m text_generator(test_prompt, max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBaseline \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model\u001b[39m}\u001b[39;00m\u001b[39m generated result: \u001b[39m\u001b[39m{\u001b[39;00mtest_prompt\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m{\u001b[39;00mresult[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBaseline \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model\u001b[39m}\u001b[39;00m\u001b[39m generated result: \u001b[39m\u001b[39m{\u001b[39;00mtest_prompt\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m{\u001b[39;00mresult[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2736\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2737\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2738\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2739\u001b[0m )\n\u001b[1;32m   2741\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:843\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    840\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[1;32m    844\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    845\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n","File \u001b[0;32m~/anaconda3/envs/HuggingFace/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"]}],"source":["#do a test prediction using the HuggingFace pipeline and this base model\n","from transformers import pipeline\n","text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","# grab a piece of text from the eval data set to use as a prompt\n","test_prompt = \"It is your only other choice\"\n","result = text_generator(test_prompt, max_length=100, num_return_sequences=1)\n","print(f\"Baseline {pretrained_model} generated result: {test_prompt}...{result[0]['generated_text']}\")\n","logger.info(f\"Baseline {pretrained_model} generated result: {test_prompt}...{result[0]['generated_text']}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()\n","eval_results = trainer.evaluate()\n","logger.info(f\"Fine-tuned {finetuned_modelname} Results: Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n","print(f\"Fine-tuned {finetuned_modelname} Results: Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n","\n","#generate text based on the fine tuned model\n","text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","result = text_generator(test_prompt, max_length=100, num_return_sequences=1)\n","print(f\"Fine-tuned {finetuned_modelname} generated result: {test_prompt}...{result[0]['generated_text']}\")\n","logger.info(f\"Fine-tuned {finetuned_modelname} generated result: {test_prompt}...{result[0]['generated_text']}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.push_to_hub()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","for batch in trainer.get_train_dataloader():\n","    break\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","batch = {k: v.to(device) for k, v in batch.items()}\n","trainer.create_optimizer()\n","\n","for _ in range(20):\n","    outputs = trainer.model(**batch)\n","    loss = outputs.loss\n","    loss.backward()\n","    trainer.optimizer.step()\n","    trainer.optimizer.zero_grad()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import evaluate\n","\n","\n","with torch.no_grad():\n","    outputs = trainer.model(**batch)\n","preds = outputs.logits\n","labels = batch[\"labels\"]\n","\n","eval_prediction = EvalPrediction(predictions=preds.cpu().numpy(), label_ids=labels.cpu().numpy())\n","compute_metrics(eval_prediction)\n","\n","\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"HuggingFace","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"36b514b738acff4061330311a03dff86b1232f3a413278e3b727ffbc7c44120f"}}},"nbformat":4,"nbformat_minor":2}
